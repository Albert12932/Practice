import pandas as pd
import numpy as np
import re
from pathlib import Path
from collections import Counter
from dateutil import parser as du_parser

import streamlit as st
import plotly.express as px
import matplotlib.pyplot as plt

#NLP –ø–∞–∫–µ—Ç—ã –∏ —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
import nltk
from nltk.corpus import stopwords
from wordcloud import WordCloud
from sklearn.feature_extraction.text import CountVectorizer

# –ì–∞—Ä–∞–Ω—Ç–∏—Ä—É–µ–º —Å—Ç–æ–ø-—Å–ª–æ–≤–∞
try:
    _ = stopwords.words("russian")
except LookupError:
    nltk.download("stopwords")

# –ö–æ–Ω—Ñ–∏–≥/–ø—É—Ç–∏
st.set_page_config(page_title="–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –æ—Ç–∑—ã–≤–æ–≤", layout="wide")
DATA_PATH = Path(r"C:\Users\shoma\OneDrive\–†–∞–±–æ—á–∏–π —Å—Ç–æ–ª\pract\out_reviews\dataset_reviews.csv")

# –í—Å–ø–æ–º–æ–≥–∞—Ç–µ–ª—å–Ω—ã–µ —Ñ—É–Ω–∫—Ü–∏–∏
RU_MONTHS = {
    1: "–Ø–Ω–≤–∞—Ä—å", 2: "–§–µ–≤—Ä–∞–ª—å", 3: "–ú–∞—Ä—Ç", 4: "–ê–ø—Ä–µ–ª—å", 5: "–ú–∞–π", 6: "–ò—é–Ω—å",
    7: "–ò—é–ª—å", 8: "–ê–≤–≥—É—Å—Ç", 9: "–°–µ–Ω—Ç—è–±—Ä—å", 10: "–û–∫—Ç—è–±—Ä—å", 11: "–ù–æ—è–±—Ä—å", 12: "–î–µ–∫–∞–±—Ä—å"
}

STOP_RU = set(stopwords.words("russian"))
CUSTOM_STOPS = {
    "–≥–æ—Å—Ç–∏–Ω–∏—Ü–∞","–æ—Ç–µ–ª—å","–Ω–æ–º–µ—Ä","–Ω–æ–º–µ—Ä–∞","–æ—Ç–∑—ã–≤","–æ—Ç–∑—ã–≤—ã","–æ—á–µ–Ω—å","–ø—Ä–∏–µ—Ö–∞–ª–∏","—Å–ø–∞—Å–∏–±–æ",
    "–±—ã–ª–∏","–±—ã–ª","–±—ã–ª–∞","—ç—Ç–æ","–≤—Å—ë","–≤—Å–µ","—Ç–∞–∫","–µ—â—ë","–Ω–∞–º","–Ω–∞—Å","–≤–∞–º","–µ—Å—Ç—å",
    "–ø—Ä–æ—Å—Ç–æ","–±—É–¥—Ç–æ","—Ç–∏–ø–∞","–≤–æ–æ–±—â–µ","–º–æ–≥–ª–∏","–º–æ–≥","–º–æ–≥–ª–∞","–±—É–¥–µ–º","—Å–≤–æ–∏—Ö","—Å–≤–æ–π","—Å–≤–æ–∏",
}
STOP_ALL = STOP_RU | CUSTOM_STOPS

RED_STEMS = [
    "–≥—Ä—è–∑–Ω", "—à—É–º", "—Ö–æ–ª–æ–¥", "–∂–∞—Ä", "–∑–∞–ø–∞—Ö", "–≤–æ–Ω",
    "—Ç–∞—Ä–∞–∫–∞–Ω", "–ø–ª–µ—Å–µ–Ω", "—Å—ã—Ä–æ—Å—Ç", "–∫–æ–Ω–¥–∏—Ü–∏–æ–Ω–µ—Ä", "–æ—Ç–æ–ø–ª–µ–Ω",
    "–ø–ª–æ—Ö", "–¥–æ–ª–≥–æ", "–º–µ–¥–ª–µ–Ω", "—É–±–æ—Ä–∫", "–ø—Ä–æ—Å—Ç—ã–Ω", "–ø–æ–ª–æ—Ç–µ–Ω",
    "–∏–Ω—Ç–µ—Ä–Ω–µ—Ç", "wifi", "–≤–∞–π—Ñ", "–ø–∞—Ä–∫–æ–≤–∫", "—Ä–µ—Å–µ–ø—à–µ–Ω", "–∑–∞—Å–µ–ª–µ–Ω", "–∑–∞–¥–µ—Ä–∂"
]

def clean_text(s: str) -> str:
    s = str(s).lower()
    s = re.sub(r"[^–∞-—è—ëa-z\s]", " ", s)
    s = re.sub(r"\s+", " ", s).strip()
    return s

def tokenize_series(series: pd.Series):
    toks = []
    for t in series.fillna("").astype(str).map(clean_text):
        toks.extend([w for w in t.split() if w not in STOP_ALL and len(w) >= 3])
    return toks

def top_bigrams(series: pd.Series, topk=10):
    corpus = series.fillna("").astype(str).map(clean_text).tolist()
    cv = CountVectorizer(
        ngram_range=(2,2),
        token_pattern=r"(?u)\b[–∞-—è—ëa-z]{3,}\b",
        stop_words=list(STOP_ALL)
    )
    if not any(len(x.split()) >= 2 for x in corpus):
        return []
    X = cv.fit_transform(corpus)
    if X.shape[1] == 0:
        return []
    freqs = X.sum(axis=0).A1
    vocab = cv.get_feature_names_out()
    pairs = list(zip(vocab, freqs))
    pairs.sort(key=lambda x: x[1], reverse=True)
    return pairs[:topk]

def has_red_issue(text: str) -> bool:
    t = clean_text(text)
    return any(stem in t for stem in RED_STEMS)

def ensure_datetime_cols(df: pd.DataFrame) -> pd.DataFrame:
    """–£–±–µ–∂–¥–∞–µ–º—Å—è, —á—Ç–æ –µ—Å—Ç—å –ì–æ–¥/–ú–µ—Å—è—Ü –∏ –∫–ª—é—á–µ–≤–æ–π datetime –¥–ª—è –≥—Ä—É–ø–ø–∏—Ä–æ–≤–æ–∫."""
    out = df.copy()
    if "–ì–æ–¥" not in out.columns or "–ú–µ—Å—è—Ü" not in out.columns:
        date_col = None
        for c in ["–î–∞—Ç–∞_ISO", "date_iso", "date", "published_date", "publishedDate", "time", "publishedTime"]:
            if c in out.columns:
                date_col = c; break
        if date_col is not None:
            dt = pd.to_datetime(out[date_col], errors="coerce")
            out["–ì–æ–¥"] = dt.dt.year
            out["–ú–µ—Å—è—Ü"] = dt.dt.month
    out["_key"] = pd.to_datetime(dict(year=out["–ì–æ–¥"], month=out["–ú–µ—Å—è—Ü"], day=1), errors="coerce")
    return out

def sentiment_from_rating(r):
    try:
        r = float(r)
    except:
        return None
    if r >= 4: return "–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ"
    if r <= 2: return "–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ"
    return "–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ"

# –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö
df = pd.read_csv(DATA_PATH, encoding="utf-8-sig")
TEXT_COL = None
for c in ["–¢–µ–∫—Å—Ç –æ—Ç–∑—ã–≤–∞", "text", "review_text", "snippet"]:
    if c in df.columns:
        TEXT_COL = c; break
if TEXT_COL is None:
    st.error("–ù–µ –Ω–∞–π–¥–µ–Ω–∞ –∫–æ–ª–æ–Ω–∫–∞ —Å —Ç–µ–∫—Å—Ç–æ–º –æ—Ç–∑—ã–≤–∞.")
    st.stop()

df = ensure_datetime_cols(df)
df["–†–µ–π—Ç–∏–Ω–≥"] = pd.to_numeric(df["–†–µ–π—Ç–∏–Ω–≥"], errors="coerce")

# –ë–æ–∫–æ–≤–∞—è –ø–∞–Ω–µ–ª—å (—Ñ–∏–ª—å—Ç—Ä—ã)
st.sidebar.header("–§–∏–ª—å—Ç—Ä—ã")
years = sorted([int(y) for y in df["–ì–æ–¥"].dropna().unique()])
year_sel = st.sidebar.multiselect("–ì–æ–¥", options=years, default=years)
months = list(range(1,13))
month_labels = [RU_MONTHS[m] for m in months]
month_map = dict(zip(month_labels, months))
month_sel_labels = st.sidebar.multiselect("–ú–µ—Å—è—Ü", options=month_labels, default=month_labels)
month_sel = [month_map[m] for m in month_sel_labels]

# –î–∏–∞–ø–∞–∑–æ–Ω —Ä–µ–π—Ç–∏–Ω–≥–∞ (–Ω–∞ –≤—Å—è–∫–∏–π)
rmin, rmax = float(np.nanmin(df["–†–µ–π—Ç–∏–Ω–≥"])), float(np.nanmax(df["–†–µ–π—Ç–∏–Ω–≥"]))
rsel = st.sidebar.slider("–§–∏–ª—å—Ç—Ä –ø–æ —Ä–µ–π—Ç–∏–Ω–≥—É", min_value=1.0, max_value=5.0, value=(1.0, 5.0), step=0.5)

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è
mask = (
    df["–ì–æ–¥"].isin(year_sel) &
    df["–ú–µ—Å—è—Ü"].isin(month_sel) &
    df["–†–µ–π—Ç–∏–Ω–≥"].between(rsel[0], rsel[1], inclusive="both")
)
dff = df.loc[mask].copy()

st.title("–ê–Ω–∞–ª–∏—Ç–∏–∫–∞ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –≥–æ—Å—Ç–∏–Ω–∏—Ü–µ")
st.caption("–ò–Ω—Ç–µ—Ä–∞–∫—Ç–∏–≤–Ω—ã–π –¥–∞—à–±–æ—Ä–¥: —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏—è –≤ –ª–µ–≤–æ–π –ø–∞–Ω–µ–ª–∏, –≤–∫–ª–∞–¥–∫–∏ –Ω–∏–∂–µ.")

# –í–∫–ª–∞–¥–∫–∏
tab1, tab2 = st.tabs(["üìä –û–±–∑–æ—Ä", "üìù –¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑"])

#–û–±–∑–æ—Ä–Ω—ã–µ –≥—Ä–∞—Ñ–∏–∫–∏ (–∫–æ—Ä–æ—Ç–∫–æ)
with tab1:
    c1, c2 = st.columns(2)
    with c1:
        # –°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –º–µ—Å—è—Ü–∞–º (–æ–±—ä–µ–¥–∏–Ω—è—è –≥–æ–¥—ã)
        monthly_avg = (
            dff.dropna(subset=["–ú–µ—Å—è—Ü","–†–µ–π—Ç–∏–Ω–≥"])
               .astype({"–ú–µ—Å—è—Ü":"int"})
               .groupby("–ú–µ—Å—è—Ü")["–†–µ–π—Ç–∏–Ω–≥"]
               .mean()
               .reindex(range(1,13))
        )
        fig = px.bar(
            monthly_avg.reset_index().rename(columns={"index":"–ú–µ—Å—è—Ü", "–†–µ–π—Ç–∏–Ω–≥":"–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥"}),
            x="–ú–µ—Å—è—Ü", y="–°—Ä–µ–¥–Ω–∏–π —Ä–µ–π—Ç–∏–Ω–≥",
            title="–°—Ä–µ–¥–Ω—è—è –æ—Ü–µ–Ω–∫–∞ –ø–æ –º–µ—Å—è—Ü–∞–º (–≤—Å–µ –≤—ã–±—Ä–∞–Ω–Ω—ã–µ –≥–æ–¥—ã)"
        )
        fig.update_xaxes(tickmode="array", tickvals=list(range(1,13)), ticktext=[RU_MONTHS[m] for m in range(1,13)])
        st.plotly_chart(fig, use_container_width=True)

    with c2:
        # –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –≥–æ–¥–∞–º
        by_year = (
            dff.dropna(subset=["–ì–æ–¥"])
               .astype({"–ì–æ–¥":"int"})
               .groupby("–ì–æ–¥").size()
               .reset_index(name="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ")
               .sort_values("–ì–æ–¥")
        )
        fig = px.bar(by_year, x="–ì–æ–¥", y="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", title="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –æ—Ç–∑—ã–≤–æ–≤ –ø–æ –≥–æ–¥–∞–º")
        st.plotly_chart(fig, use_container_width=True)

    # –ü–∞–π-—á–∞—Ä—Ç: –ø–æ–∑–∏—Ç–∏–≤/–Ω–µ–π—Ç—Ä–∞–ª/–Ω–µ–≥–∞—Ç–∏–≤
    cats = dff["–†–µ–π—Ç–∏–Ω–≥"].apply(sentiment_from_rating)
    counts = pd.Series(cats).value_counts().reindex(["–ü–æ–∑–∏—Ç–∏–≤–Ω—ã–µ","–ù–µ–π—Ç—Ä–∞–ª—å–Ω—ã–µ","–ù–µ–≥–∞—Ç–∏–≤–Ω—ã–µ"]).fillna(0)
    fig = px.pie(values=counts.values, names=counts.index, title="–°—Ç—Ä—É–∫—Ç—É—Ä–∞ –æ—Ç–∑—ã–≤–æ–≤")
    st.plotly_chart(fig, use_container_width=True)

    st.markdown("---")

    colA, colB = st.columns(2)

    #–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤
    with colA:
        if dff["–†–µ–π—Ç–∏–Ω–≥"].notna().sum() > 0:
            fig = px.histogram(dff, x="–†–µ–π—Ç–∏–Ω–≥", nbins=5, title="–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ–π—Ç–∏–Ω–≥–æ–≤ (1‚Äì5)")
            fig.update_layout(xaxis=dict(tickmode="array", tickvals=[1,2,3,4,5]))
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("–ù–µ—Ç –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ä–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Ä–µ–π—Ç–∏–Ω–≥–æ–≤.")

    #–î–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–æ–≤
    with colB:
        dff["–î–ª–∏–Ω–∞_—Å–ª–æ–≤"] = dff[TEXT_COL].fillna("").apply(lambda s: len(str(s).split()))
        bins = [0, 25, 50, 100, 10_000]
        labels = ["0‚Äì25", "26‚Äì50", "51‚Äì100", "101+"]

        dff["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª–∏–Ω—ã"] = pd.cut(dff["–î–ª–∏–Ω–∞_—Å–ª–æ–≤"], bins=bins, labels=labels, right=True, include_lowest=True)
        counts = dff["–ö–∞—Ç–µ–≥–æ—Ä–∏—è –¥–ª–∏–Ω—ã"].value_counts().reindex(labels, fill_value=0).reset_index()
        counts.columns = ["–î–∏–∞–ø–∞–∑–æ–Ω", "–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ"]

        fig = px.bar(counts, x="–î–∏–∞–ø–∞–∑–æ–Ω", y="–ö–æ–ª–∏—á–µ—Å—Ç–≤–æ", title="–î–ª–∏–Ω–∞ –æ—Ç–∑—ã–≤–æ–≤ (–≤ —Å–ª–æ–≤–∞—Ö)")
        st.plotly_chart(fig, use_container_width=True)

    st.subheader("–ü—Ä–æ—Å–º–æ—Ç—Ä –æ—Ç–∑—ã–≤–æ–≤ (–ø–µ—Ä–≤—ã–µ 200)")
    st.dataframe(dff[[ "–†–µ–π—Ç–∏–Ω–≥", "–ì–æ–¥", "–ú–µ—Å—è—Ü", TEXT_COL ]].head(200))

#–¢–µ–∫—Å—Ç–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑
with tab2:
    st.subheader("–û–±–ª–∞–∫–∞ —Å–ª–æ–≤ (–ø–æ–∑–∏—Ç–∏–≤ ‚â•4‚òÖ / –Ω–µ–≥–∞—Ç–∏–≤ ‚â§2‚òÖ)")
    colL, colR = st.columns(2)

    pos_df = dff[dff["–†–µ–π—Ç–∏–Ω–≥"] >= 4]
    neg_df = dff[dff["–†–µ–π—Ç–∏–Ω–≥"] <= 2]

    def texts_to_corpus(series: pd.Series) -> str:
        cleaned = series.fillna("").astype(str).map(clean_text)
        tokens = []
        for t in cleaned:
            tokens.extend([w for w in t.split() if w not in STOP_ALL and len(w) > 2])
        return " ".join(tokens)

    with colL:
        corpus_pos = texts_to_corpus(pos_df[TEXT_COL])
        if len(corpus_pos) > 0:
            wc_pos = WordCloud(width=900, height=540, background_color="white", collocations=False, prefer_horizontal=0.9).generate(corpus_pos)
            fig_wc1, ax1 = plt.subplots(figsize=(8,5))
            ax1.imshow(wc_pos, interpolation="bilinear")
            ax1.axis("off")
            st.pyplot(fig_wc1, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –ø–æ–∑–∏—Ç–∏–≤–Ω–æ–≥–æ –æ–±–ª–∞–∫–∞.")

    with colR:
        corpus_neg = texts_to_corpus(neg_df[TEXT_COL])
        if len(corpus_neg) > 0:
            wc_neg = WordCloud(width=900, height=540, background_color="white", collocations=False, prefer_horizontal=0.9).generate(corpus_neg)
            fig_wc2, ax2 = plt.subplots(figsize=(8,5))
            ax2.imshow(wc_neg, interpolation="bilinear")
            ax2.axis("off")
            st.pyplot(fig_wc2, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –Ω–µ–≥–∞—Ç–∏–≤–Ω–æ–≥–æ –æ–±–ª–∞–∫–∞.")

    st.markdown("---")
    st.subheader("–¢–û–ü-—Å–ª–æ–≤–∞ –∏ –±–∏–≥—Ä–∞–º–º—ã")

    topk = st.slider("TOP-N –¥–ª—è —á–∞—Å—Ç–æ—Ç–Ω—ã—Ö —Å–ø–∏—Å–∫–æ–≤/–±–∏–≥—Ä–∞–º–º", min_value=5, max_value=30, value=10, step=1)

    #–¢–û–ü-—Å–ª–æ–≤–∞ (–ø–æ–∑–∏—Ç–∏–≤/–Ω–µ–≥–∞—Ç–∏–≤)
    col1, col2 = st.columns(2)
    with col1:
        pos_tokens = tokenize_series(pos_df[TEXT_COL])
        pos_top = Counter(pos_tokens).most_common(topk)
        if pos_top:
            words, counts = zip(*pos_top)
            fig = px.bar(x=list(words), y=list(counts), title=f"–¢–û–ü-{topk} —Å–ª–æ–≤ ‚Äî –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ")
            fig.update_layout(xaxis_title="–°–ª–æ–≤–∞", yaxis_title="–ß–∞—Å—Ç–æ—Ç–∞")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¢–û–ü-—Å–ª–æ–≤ (–ø–æ–∑–∏—Ç–∏–≤).")

    with col2:
        neg_tokens = tokenize_series(neg_df[TEXT_COL])
        neg_top = Counter(neg_tokens).most_common(topk)
        if neg_top:
            words, counts = zip(*neg_top)
            fig = px.bar(x=list(words), y=list(counts), title=f"–¢–û–ü-{topk} —Å–ª–æ–≤ ‚Äî –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ")
            fig.update_layout(xaxis_title="–°–ª–æ–≤–∞", yaxis_title="–ß–∞—Å—Ç–æ—Ç–∞")
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –¢–û–ü-—Å–ª–æ–≤ (–Ω–µ–≥–∞—Ç–∏–≤).")

    #–¢–û–ü-–±–∏–≥—Ä–∞–º–º (–ø–æ–∑–∏—Ç–∏–≤/–Ω–µ–≥–∞—Ç–∏–≤)
    col3, col4 = st.columns(2)
    with col3:
        pos_bi = top_bigrams(pos_df[TEXT_COL], topk=topk)
        if pos_bi:
            labels, vals = zip(*pos_bi)
            fig = px.bar(x=list(labels), y=list(vals), title=f"–¢–û–ü-{topk} –±–∏–≥—Ä–∞–º–º ‚Äî –ø–æ–∑–∏—Ç–∏–≤–Ω—ã–µ")
            fig.update_layout(xaxis_title="–ë–∏–≥—Ä–∞–º–º—ã", yaxis_title="–ß–∞—Å—Ç–æ—Ç–∞")
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∏–≥—Ä–∞–º–º (–ø–æ–∑–∏—Ç–∏–≤).")

    with col4:
        neg_bi = top_bigrams(neg_df[TEXT_COL], topk=topk)
        if neg_bi:
            labels, vals = zip(*neg_bi)
            fig = px.bar(x=list(labels), y=list(vals), title=f"–¢–û–ü-{topk} –±–∏–≥—Ä–∞–º–º ‚Äî –Ω–µ–≥–∞—Ç–∏–≤–Ω—ã–µ")
            fig.update_layout(xaxis_title="–ë–∏–≥—Ä–∞–º–º—ã", yaxis_title="–ß–∞—Å—Ç–æ—Ç–∞")
            fig.update_xaxes(tickangle=45)
            st.plotly_chart(fig, use_container_width=True)
        else:
            st.info("–ù–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –±–∏–≥—Ä–∞–º–º (–Ω–µ–≥–∞—Ç–∏–≤).")
